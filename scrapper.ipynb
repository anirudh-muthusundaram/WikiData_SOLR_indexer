{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrpaing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, FeatureNotFound\n",
    "\n",
    "# UB IT name based user agent\n",
    "user = \"amuthusu/IR (https://www.linkedin.com/in/anirudhmuthusundaram/; anirudhms247@gmail.com)\"\n",
    "\n",
    "# User agent Assignment\n",
    "wikipedia.set_user_agent(user)\n",
    "\n",
    "# API endpoint\n",
    "url = \"https://www.mediawiki.org/w/api.php\"\n",
    "headers = {\n",
    "    \"User-Agent\": user\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Common cold\n",
      "PageError for title: Pogona\n",
      "PageError for title: Infection\n",
      "PageError for title: Diseases of the foot\n",
      "PageError for title: Nonsense mutation\n",
      "PageError for title: Tetter\n",
      "PageError for title: Lupus\n",
      "PageError for title: Nobel disease\n",
      "PageError for title: Bronchus\n",
      "PageError for title: Chagas disease\n",
      "PageError for title: ALS\n",
      "PageError for title: Batten disease\n",
      "PageError for title: Human body\n",
      "PageError for title: List of rye diseases\n",
      "PageError for title: Kuru (disease)\n",
      "PageError for title: Tomato\n",
      "PageError for title: Canavan disease\n",
      "PageError for title: Longan\n",
      "PageError for title: Gallstone\n",
      "PageError for title: Common nighthawk\n",
      "PageError for title: Herd immunity\n",
      "PageError for title: Canada\n",
      "PageError for title: Sustainable Development Goal 3\n",
      "PageError for title: Average human height by country\n",
      "PageError for title: Mortality rate\n",
      "PageError for title: Statistics of the COVID-19 pandemic in Brazil\n",
      "PageError for title: Allied health professions\n",
      "PageError for title: Environmental health\n",
      "PageError for title: Population growth\n",
      "PageError for title: China\n",
      "PageError for title: Metro Manila\n",
      "PageError for title: Economy of Canada\n",
      "PageError for title: World Economic Forum\n",
      "PageError for title: Benin\n",
      "PageError for title: Rape statistics\n",
      "PageError for title: Norway\n",
      "PageError for title: Obesity\n",
      "PageError for title: Demographics of Africa\n",
      "PageError for title: Healthcare in the United Kingdom\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Health': ['Common diseases','Global health statistics','Mental health trends'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "h_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Health.csv\"\n",
    "h_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Health = pd.read_csv(\"./Data/Health.csv\")\n",
    "Health.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Drought\n",
      "PageError for title: Climate of Mars\n",
      "PageError for title: Arctic\n",
      "PageError for title: Indus River\n",
      "PageError for title: Polar bear\n",
      "PageError for title: Maximum Ride\n",
      "PageError for title: Deforestation\n",
      "PageError for title: John Hagee\n",
      "PageError for title: Hurricane Sandy\n",
      "PageError for title: Frank Luntz\n",
      "PageError for title: James Spann\n",
      "PageError for title: Environmental issues\n",
      "PageError for title: Richard Branson\n",
      "PageError for title: Greta Thunberg\n",
      "PageError for title: R-410A\n",
      "PageError for title: El Ni√±o\n",
      "PageError for title: Rat snake\n",
      "PageError for title: Steven Tyler\n",
      "PageError for title: R-454B\n",
      "PageError for title: Shale gas\n",
      "PageError for title: Environmental science\n",
      "PageError for title: Titanic II (film)\n",
      "PageError for title: Pollution\n",
      "PageError for title: An Inconvenient Truth\n",
      "PageError for title: Water scarcity\n",
      "PageError for title: Swami Sundaranand\n",
      "PageError for title: World Economic Forum\n",
      "PageError for title: Al Gore\n",
      "PageError for title: DeSmog\n",
      "PageError for title: John Christy\n",
      "PageError for title: Ice age\n",
      "PageError for title: The Daily Sceptic\n",
      "PageError for title: Richard Lindzen\n",
      "PageError for title: American pika\n",
      "PageError for title: Earth Day\n",
      "PageError for title: Virgin River\n",
      "PageError for title: Formosan black bear\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Environment': ['Global warming','Endangered species','Deforestation rates'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "env_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Environment.csv\"\n",
    "env_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "environment = pd.read_csv(\"./Data/Environment.csv\")\n",
    "environment.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Transhumanism\n",
      "PageError for title: Special Competitive Studies Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: 3D printing\n",
      "PageError for title: Wendell Wallach\n",
      "PageError for title: Daphne Koller\n",
      "PageError for title: Loitering munition\n",
      "PageError for title: Timeline of historic inventions\n",
      "PageError for title: SD-WAN\n",
      "PageError for title: Bioconvergence\n",
      "PageError for title: Vertical farming\n",
      "PageError for title: Unity Technologies\n",
      "PageError for title: Flying car\n",
      "PageError for title: Accenture\n",
      "PageError for title: Gavin Andresen\n",
      "PageError for title: Superhuman\n",
      "PageError for title: Upgrade\n",
      "PageError for title: Designer baby\n",
      "PageError for title: BICO Group\n",
      "PageError for title: Beggars in Spain\n",
      "PageError for title: Metaliteracy\n",
      "PageError for title: Anthony Dunne\n",
      "PageError for title: Transrapid\n",
      "PageError for title: Nanoengineering\n",
      "PageError for title: Davfs2\n",
      "PageError for title: Schmidt Futures\n",
      "PageError for title: DevOps\n",
      "PageError for title: Nick Bostrom\n",
      "PageError for title: Computer vision\n",
      "PageError for title: AI boom\n",
      "PageError for title: ServiceNow\n",
      "PageError for title: Generative artificial intelligence\n",
      "PageError for title: Ai Weiwei\n",
      "PageError for title: Meta AI\n",
      "PageError for title: Roko's basilisk\n",
      "PageError for title: Dong Xian\n",
      "PageError for title: Terah Lyons\n",
      "PageError for title: Deepfake\n",
      "PageError for title: Amy Karle\n",
      "PageError for title: Nvidia GTC\n",
      "PageError for title: Neuralink\n",
      "PageError for title: A.I. Insight forums\n",
      "PageError for title: Eric Horvitz\n",
      "PageError for title: Digital literacy\n",
      "PageError for title: IP camera\n",
      "PageError for title: XPeng\n",
      "PageError for title: Project Jupyter\n",
      "PageError for title: Amazon Alexa\n",
      "PageError for title: Camera\n",
      "PageError for title: Fa√ßade (video game)\n",
      "PageError for title: Moore's law\n",
      "PageError for title: Arms race\n",
      "PageError for title: Song-Chun Zhu\n",
      "PageError for title: Civilization IV\n",
      "PageError for title: 49th G7 summit\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Technology': ['Emerging technologies','AI advancements', 'Bitcoin'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "tech_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Technology.csv\"\n",
    "tech_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Technology = pd.read_csv(\"./Data/Technology.csv\")\n",
    "Technology.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: S&P 500\n",
      "PageError for title: Russell 1000 Index\n",
      "PageError for title: Russell 3000 Index\n",
      "PageError for title: Nasdaq-100\n",
      "PageError for title: ISO 9000\n",
      "PageError for title: Danone\n",
      "PageError for title: Abnormal return\n",
      "PageError for title: Money market\n",
      "PageError for title: KOSPI\n",
      "PageError for title: Nifty Fifty\n",
      "PageError for title: Wilshire 5000\n",
      "PageError for title: CAC 40\n",
      "PageError for title: Dow Jones\n",
      "PageError for title: Russell 2500 Index\n",
      "PageError for title: P/B ratio\n",
      "PageError for title: AEX index\n",
      "PageError for title: Jordan Belfort\n",
      "PageError for title: S&P 600\n",
      "PageError for title: Hedge fund\n",
      "PageError for title: Shutterstock\n",
      "PageError for title: Mopar\n",
      "PageError for title: Rights issue\n",
      "PageError for title: EURO STOXX 50\n",
      "PageError for title: PSI-20\n",
      "PageError for title: Marc Chaikin\n",
      "PageError for title: Hero MotoCorp\n",
      "PageError for title: Ray Dalio\n",
      "PageError for title: Maxar Technologies\n",
      "PageError for title: Capitalism\n",
      "PageError for title: Enron\n",
      "PageError for title: ASRock\n",
      "PageError for title: Business valuation\n",
      "PageError for title: Bill Ackman\n",
      "PageError for title: Inventory turnover\n",
      "PageError for title: .41 Remington Magnum\n",
      "PageError for title: Black market\n",
      "PageError for title: Walmart\n",
      "PageError for title: Robert J. Shiller\n",
      "PageError for title: Economy of China\n",
      "PageError for title: Dodge Charger\n",
      "PageError for title: Smith & Wesson M&P15\n",
      "PageError for title: Enron scandal\n",
      "PageError for title: Financial planner\n",
      "PageError for title: EDreams\n",
      "PageError for title: Jason Statham\n",
      "PageError for title: Amer Sports\n",
      "PageError for title: Economy of Canada\n",
      "PageError for title: Earnings guidance\n",
      "PageError for title: Resideo\n",
      "PageError for title: LiveChat Software\n",
      "PageError for title: Stellantis\n",
      "PageError for title: Mark Hulbert\n",
      "PageError for title: Strategic management\n",
      "PageError for title: Honda CR-X\n",
      "PageError for title: Economics Job Market Rumors\n",
      "PageError for title: Steve Jobs\n",
      "PageError for title: Nonfarm payrolls\n",
      "PageError for title: Economy of Canada\n",
      "PageError for title: Early 2000s recession\n",
      "PageError for title: Deseret Industries\n",
      "PageError for title: Wegmans\n",
      "PageError for title: Michael Spence\n",
      "PageError for title: Economic migrant\n",
      "PageError for title: Financial Times\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Economy': ['Stock market performance','Job markets','Cryptocurrency trends'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "eco_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Economy.csv\"\n",
    "eco_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Economy = pd.read_csv(\"./Data/Economy.csv\")\n",
    "Economy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Music\n",
      "PageError for title: Record producer\n",
      "PageError for title: Pop music\n",
      "PageError for title: Tim Farriss\n",
      "PageError for title: Andrew Farriss\n",
      "PageError for title: Simon Cowell\n",
      "PageError for title: C-pop\n",
      "PageError for title: MTV\n",
      "PageError for title: Goa trance\n",
      "PageError for title: NF (rapper)\n",
      "PageError for title: 2023 in Latin music\n",
      "PageError for title: G Flip\n",
      "PageError for title: Sony Music\n",
      "PageError for title: Ang√®le (singer)\n",
      "PageError for title: Tommy Emmanuel\n",
      "PageError for title: Alicia Keys\n",
      "PageError for title: Tina Arena\n",
      "PageError for title: Tones and I\n",
      "PageError for title: Thom Yorke\n",
      "PageError for title: Gangnam Style\n",
      "PageError for title: Sid Sriram\n",
      "PageError for title: Blackout Tuesday\n",
      "PageError for title: Music publisher\n",
      "PageError for title: Midnight Oil\n",
      "PageError for title: UK Singles Chart\n",
      "PageError for title: Lil' Flip\n",
      "PageError for title: Russell Simmons\n",
      "PageError for title: MAMA Awards\n",
      "PageError for title: Music of Turkey\n",
      "PageError for title: 1989 (album)\n",
      "PageError for title: Kris Kross\n",
      "PageError for title: List of programs broadcast by MTV\n",
      "PageError for title: Taylor Swift\n",
      "PageError for title: B Praak\n",
      "PageError for title: Savage Garden\n",
      "PageError for title: 2023 in Philippine music\n",
      "PageError for title: Clive Davis\n",
      "PageError for title: Martin Atkins\n",
      "PageError for title: Taeyang\n",
      "PageError for title: 2001 MTV Video Music Awards\n",
      "PageError for title: Clarence Avant\n",
      "PageError for title: Anu Malik\n",
      "PageError for title: Haddaway\n",
      "PageError for title: Lyor Cohen\n",
      "PageError for title: Daddy Yankee\n",
      "PageError for title: Kim Jong-kook\n",
      "PageError for title: Lucian Grainge\n",
      "PageError for title: Ron Perry (music)\n",
      "PageError for title: Golden Disc Awards\n",
      "PageError for title: Entertainment industry during World War II\n",
      "PageError for title: Tips Industries\n",
      "PageError for title: Tommy Mottola\n",
      "PageError for title: Dream Theater discography\n",
      "PageError for title: Music of Rwanda\n",
      "PageError for title: Mark Volman\n",
      "PageError for title: Cultural tourism\n",
      "PageError for title: Cultural relativism\n",
      "PageError for title: Cultural Revolution\n",
      "PageError for title: Cultural competence\n",
      "PageError for title: World Heritage Site\n",
      "PageError for title: Culture during the Cold War\n",
      "PageError for title: Sinosphere\n",
      "PageError for title: List of subcultures\n",
      "PageError for title: Pop music\n",
      "PageError for title: Generation\n",
      "PageError for title: Culture of India\n",
      "PageError for title: Pok√©mon\n",
      "PageError for title: Chuseok\n",
      "PageError for title: Tug of war\n",
      "PageError for title: Fernando Amorsolo\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Entertainment': ['Music industry','Popular cultural events','Streaming platforms'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "entr_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Entertainment.csv\"\n",
    "entr_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Entertainment = pd.read_csv(\"./Data/Entertainment.csv\")\n",
    "Entertainment.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Bet9ja\n",
      "PageError for title: Des Lynam\n",
      "PageError for title: Sportsbook\n",
      "PageError for title: Intersport\n",
      "PageError for title: Tom Rinaldi\n",
      "PageError for title: 2007 Rugby World Cup\n",
      "PageError for title: AEW Rampage\n",
      "PageError for title: One Sports\n",
      "PageError for title: Postgame\n",
      "PageError for title: Nancy Faeser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Calvin Murphy\n",
      "PageError for title: Gay Games\n",
      "PageError for title: Halifax, Nova Scotia\n",
      "PageError for title: Eurosport\n",
      "PageError for title: 2022 in Mexico\n",
      "PageError for title: Moncton\n",
      "PageError for title: ESPN on ABC\n",
      "PageError for title: Super Bowl\n",
      "PageError for title: Amanda Beard\n",
      "PageError for title: Megaphone\n",
      "PageError for title: BOK Center\n",
      "PageError for title: Mir (television company)\n",
      "PageError for title: Megaproject\n",
      "PageError for title: Peter Warrick\n",
      "PageError for title: Todd Boehly\n",
      "PageError for title: Pickleball\n",
      "PageError for title: 1965 Major League Baseball draft\n",
      "PageError for title: Rogue Invitational\n",
      "PageError for title: 1987 Major League Baseball draft\n",
      "PageError for title: Abbie Hoffman\n",
      "PageError for title: 2010 June rugby union tests\n",
      "PageError for title: Zach Lowe\n",
      "PageError for title: Predictive analytics\n",
      "PageError for title: Tony Khan\n",
      "PageError for title: Druski\n",
      "PageError for title: Erika Nardini\n",
      "PageError for title: Sam Hinkie\n",
      "PageError for title: Cynthia Frelund\n",
      "PageError for title: Analytics (ice hockey)\n",
      "PageError for title: Godzilla (2014 film)\n",
      "PageError for title: Kyle Dubas\n",
      "PageError for title: Suzy Kolber\n",
      "PageError for title: 1998 NFC Championship Game\n",
      "PageError for title: Tim Chartier\n",
      "PageError for title: Alison Lukan\n",
      "PageError for title: Baseball Prospectus\n",
      "PageError for title: Axon Sports\n",
      "PageError for title: Nike, Inc.\n",
      "PageError for title: His & Hers (TV program)\n",
      "PageError for title: Dan Abrams\n",
      "PageError for title: Canada\n",
      "PageError for title: FloSports\n",
      "PageError for title: Zack Scott\n",
      "PageError for title: CrazyGames\n",
      "PageError for title: Chaim Bloom\n",
      "PageError for title: Playtech\n",
      "PageError for title: 1988 Summer Olympics medal table\n",
      "PageError for title: Ted Lasso\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Sports': ['Major sporting events','Sports analytics','Sports History'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "sprs_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Sports.csv\"\n",
    "sprs_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Sports = pd.read_csv(\"./Data/Sports.csv\")\n",
    "Sports.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Independent politician\n",
      "PageError for title: 1940 United States presidential election\n",
      "PageError for title: Women's suffrage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: 1944 United States presidential election\n",
      "PageError for title: Gabon\n",
      "PageError for title: Political views of Generation Z\n",
      "PageError for title: Green party\n",
      "PageError for title: Political repression\n",
      "PageError for title: Amit Shah\n",
      "PageError for title: 2004 Indian general election\n",
      "PageError for title: Accountability\n",
      "PageError for title: Centrism\n",
      "PageError for title: 2024 Salvadoran general election\n",
      "PageError for title: Aftermath of World War I\n",
      "PageError for title: 1952 United States presidential election\n",
      "PageError for title: Elections in Chile\n",
      "PageError for title: 1956 United States presidential election\n",
      "PageError for title: 2022 United States elections\n",
      "PageError for title: 2024 United States House of Representatives elections\n",
      "PageError for title: Revolutions of 1989\n",
      "PageError for title: 1994 South African general election\n",
      "PageError for title: 2022 United States Senate elections\n",
      "PageError for title: 1864 United States presidential election\n",
      "PageError for title: 2009 Indian general election\n",
      "PageError for title: 2022 Indian presidential election\n",
      "PageError for title: Lists of political office-holders in East Germany\n",
      "PageError for title: Big lie\n",
      "PageError for title: Shas\n",
      "PageError for title: 2004 United States presidential election\n",
      "PageError for title: 2023 United States elections\n",
      "PageError for title: 1976 United States presidential election\n",
      "PageError for title: 2022 United States House of Representatives elections\n",
      "PageError for title: Generation Jones\n",
      "PageError for title: Richard R. Nelson\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Politics': ['World Political Elections','Public policy analysis','International relations'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "plts_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Politics.csv\"\n",
    "plts_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Politics = pd.read_csv(\"./Data/Politics.csv\")\n",
    "Politics.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Gender inequality in India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Digital literacy\n",
      "PageError for title: Education in Mali\n",
      "PageError for title: Manor, India\n",
      "PageError for title: World Literacy Foundation\n",
      "PageError for title: Munger district\n",
      "PageError for title: Financial literacy\n",
      "PageError for title: Malvi language\n",
      "PageError for title: River Gee County\n",
      "PageError for title: Kachhi District\n",
      "PageError for title: Banga, India\n",
      "PageError for title: Bomdila\n",
      "PageError for title: Female education\n",
      "PageError for title: Higher education\n",
      "PageError for title: XHamster\n",
      "PageError for title: Bloom's taxonomy\n",
      "PageError for title: Dark academia\n",
      "PageError for title: Title IX\n",
      "PageError for title: Scott Plous\n",
      "PageError for title: BJA Education\n",
      "PageError for title: Grade retention\n",
      "PageError for title: Generation X\n",
      "PageError for title: Google Classroom\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Education': ['Literacy rates','Education trends','Online Education'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "edcs_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Education.csv\"\n",
    "edcs_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Education = pd.read_csv(\"./Data/Education.csv\")\n",
    "Education.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Zaan\n",
      "PageError for title: Heho Airport\n",
      "PageError for title: Hiroshima\n",
      "PageError for title: Sipalay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Big Sur\n",
      "PageError for title: √éle-de-France\n",
      "PageError for title: Kanyam\n",
      "PageError for title: Belize\n",
      "PageError for title: Sex tourism\n",
      "PageError for title: Boracay\n",
      "PageError for title: Axum\n",
      "PageError for title: Tourist attractions in Vienna\n",
      "PageError for title: Disneyland Paris\n",
      "PageError for title: Camm Morton\n",
      "PageError for title: Overtourism\n",
      "PageError for title: Dalaman Airport\n",
      "PageError for title: Tourism in Egypt\n",
      "PageError for title: Top Withens\n",
      "PageError for title: CityPASS\n",
      "PageError for title: Ubud\n",
      "PageError for title: Elephanta Island\n",
      "PageError for title: Warsaw\n",
      "PageError for title: Funchal\n",
      "PageError for title: Cliffs of Moher\n",
      "PageError for title: Gay village\n",
      "PageError for title: Ko Samui\n",
      "PageError for title: Space tourism\n",
      "PageError for title: Riviera Maya\n",
      "PageError for title: Pai, Thailand\n",
      "PageError for title: Spiderhead\n",
      "PageError for title: Megijima\n",
      "PageError for title: Positano\n",
      "PageError for title: Colca Canyon\n",
      "PageError for title: Victoria Falls\n",
      "PageError for title: Porto Flavia\n",
      "PageError for title: Metro Manila\n",
      "PageError for title: Visa requirements for Indian citizens\n",
      "PageError for title: Malaysia Airlines Flight 370\n",
      "PageError for title: Flightradar24\n",
      "PageError for title: United Airlines\n",
      "PageError for title: China Airlines\n",
      "PageError for title: Spirit Airlines\n",
      "PageError for title: Frontier Airlines\n",
      "PageError for title: Northwest Airlines\n",
      "PageError for title: EasyJet\n",
      "PageError for title: Northwest Airlines Flight 255\n",
      "PageError for title: Flight attendant\n",
      "PageError for title: British Airways\n",
      "PageError for title: Airfare\n",
      "PageError for title: Air Astana\n",
      "PageError for title: US Airways\n",
      "PageError for title: American Airlines Flight 965\n",
      "PageError for title: El Al\n",
      "PageError for title: ATA 100\n",
      "PageError for title: Skybus Airlines\n",
      "PageError for title: MakeMyTrip\n",
      "PageError for title: Antonov An-12\n",
      "PageError for title: South African Airways\n",
      "PageError for title: Airlink\n",
      "PageError for title: United Airlines Flight 328\n",
      "PageError for title: Airbus A318\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Travel': ['Top tourist destinations','Airline industry data','Travel trends'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "Trvl_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Travel.csv\"\n",
    "Trvl_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Travel = pd.read_csv(\"./Data/Travel.csv\")\n",
    "Travel.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Cereal\n",
      "PageError for title: Agriculture in India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageError for title: Organic farming\n",
      "PageError for title: Alfalfa\n",
      "PageError for title: Corn production in the United States\n",
      "PageError for title: Vertical farming\n",
      "PageError for title: Pearl millet\n",
      "PageError for title: Sesame\n",
      "PageError for title: Proso millet\n",
      "PageError for title: Pigeon pea\n",
      "PageError for title: Agriculture in South Africa\n",
      "PageError for title: Injera\n",
      "PageError for title: Food security\n",
      "PageError for title: Almond\n",
      "PageError for title: Ginger\n",
      "PageError for title: Pumpkin\n",
      "PageError for title: Shiso\n",
      "PageError for title: Machine learning\n",
      "PageError for title: Yuan Longping\n",
      "PageError for title: Pollination\n",
      "PageError for title: Farmall\n",
      "PageError for title: Hop production in the United States\n",
      "PageError for title: Economy of Canada\n",
      "PageError for title: Haber process\n",
      "PageError for title: Q (disambiguation)\n",
      "PageError for title: Isabela (province)\n",
      "PageError for title: Greer County, Oklahoma\n",
      "PageError for title: Clemson University\n",
      "PageError for title: Romania\n",
      "PageError for title: Economy of China\n",
      "PageError for title: Industrial Revolution\n",
      "PageError for title: 2018 British Isles heatwave\n",
      "PageError for title: Nutmeg\n",
      "PageError for title: El Ni√±o\n",
      "PageError for title: Belize\n",
      "PageError for title: Inflation\n",
      "PageError for title: Chaos theory\n",
      "PageError for title: Gilded Age\n",
      "PageError for title: German wine\n",
      "PageError for title: Fidel Castro\n",
      "PageError for title: Ricky Ponting\n",
      "PageError for title: Food security\n",
      "PageError for title: Soup kitchen\n",
      "PageError for title: Agrifood systems\n",
      "PageError for title: Jennifer Clapp\n",
      "PageError for title: Midday Meal Scheme\n",
      "PageError for title: United Nations\n",
      "PageError for title: Yuan Longping\n",
      "PageError for title: Food desert\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and subtopics\n",
    "title = {\n",
    "    'Food': ['Crop yield statistics','Global hunger and food security','Top World Dishes'],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Each and Every Heading and Side Headings related to the title gets looped.\n",
    "# Webpages with unique titles and Summary over 250 characters are chosen.\n",
    "# More than 550 documents are set as a limit to extract information.\n",
    "# Webpages are appended onto a list that later can be used to get a DataFrame.\n",
    "# Wikipedia Disambiguation, HTTP timeout and Page errors are handled to enhance the process.\n",
    "for heading, side_heading in title.items():\n",
    "    short_summary_count = 0\n",
    "    unique_titles = set()\n",
    "\n",
    "    for iterate in side_heading:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        search_results = wikipedia.search(iterate, results=550)\n",
    "        for webpage_heading in search_results:\n",
    "            try:\n",
    "                if webpage_heading in unique_titles:\n",
    "                    continue\n",
    "                page = wikipedia.page(webpage_heading)\n",
    "                summary = page.summary\n",
    "                if len(summary) >= 250:\n",
    "                    data_list.append({\n",
    "                        'Topic': heading,\n",
    "                        'Title': page.title,\n",
    "                        'Summary': summary,\n",
    "                        'URL': page.url,\n",
    "                        'Revision ID': page.revision_id  \n",
    "                    })\n",
    "                    unique_titles.add(webpage_heading)\n",
    "                    if len(summary) < 250:\n",
    "                        short_summary_count += 1\n",
    "\n",
    "                    if len(unique_titles) >= 550:\n",
    "                        break\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                continue\n",
    "            except wikipedia.exceptions.HTTPTimeoutError as e:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                print(f\"PageError for title: {webpage_heading}\")\n",
    "                continue\n",
    "\n",
    "        if len(unique_titles) >= 550:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        550 non-null    object\n",
      " 1   Title        550 non-null    object\n",
      " 2   Summary      550 non-null    object\n",
      " 3   URL          550 non-null    object\n",
      " 4   Revision ID  550 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV from the DataFrame\n",
    "food_df = pd.DataFrame(data_list)\n",
    "csv_file_name = \"./Data/Food.csv\"\n",
    "food_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Check with the csv file.\n",
    "Food = pd.read_csv(\"./Data/Food.csv\")\n",
    "Food.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
